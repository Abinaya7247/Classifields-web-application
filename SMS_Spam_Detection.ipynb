{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3myKIhHP8lL6mS/o2hlGg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abinaya7247/Classifields-web-application/blob/main/SMS_Spam_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2-2x-1GdFclc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f14108-32e1-442d-cee1-daa3866ab710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Accuracy: 0.9820627802690582\n",
            "\n",
            "üìä Confusion Matrix:\n",
            " [[954  11]\n",
            " [  9 141]]\n",
            "\n",
            "üìù Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       965\n",
            "           1       0.93      0.94      0.93       150\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.96      0.96      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n",
            "üö® Spam\n",
            "‚úÖ Not Spam\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('spam.csv', encoding='latin-1')[['v1', 'v2']]\n",
        "data.columns = ['label', 'message']\n",
        "\n",
        "# Convert spam/ham to 0/1\n",
        "data['label_num'] = data['label'].map({'ham':0, 'spam':1})\n",
        "\n",
        "# Text preprocessing\n",
        "ps = PorterStemmer()\n",
        "corpus = []\n",
        "for msg in data['message']:\n",
        "    msg = re.sub('[^a-zA-Z]', ' ', msg)  # keep only letters\n",
        "    msg = msg.lower()                    # lowercase\n",
        "    msg = msg.split()                    # split words\n",
        "    msg = [ps.stem(word) for word in msg if word not in stopwords.words('english')]\n",
        "    msg = ' '.join(msg)\n",
        "    corpus.append(msg)\n",
        "\n",
        "# Convert text to features\n",
        "cv = CountVectorizer(max_features=2500)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "y = data['label_num'].values\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"‚úÖ Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nüìä Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nüìù Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Test custom SMS\n",
        "def detect_spam(message):\n",
        "    msg = re.sub('[^a-zA-Z]', ' ', message)\n",
        "    msg = msg.lower()\n",
        "    msg = msg.split()\n",
        "    msg = [ps.stem(word) for word in msg if word not in stopwords.words('english')]\n",
        "    msg = ' '.join(msg)\n",
        "    vector = cv.transform([msg]).toarray()\n",
        "    prediction = model.predict(vector)\n",
        "    return \"üö® Spam\" if prediction == [1] else \"‚úÖ Not Spam\"\n",
        "\n",
        "# Test\n",
        "print(detect_spam(\"Congratulations! You've won $1000. Claim now!\"))\n",
        "print(detect_spam(\"Hello bro, what's up?\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "55ppKYa1FreY"
      }
    }
  ]
}